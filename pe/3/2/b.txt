# PE3.2 Controlled Experiments

Controlled experiments are crucial for validating performance improvements and understanding the variables that impact system efficiency. This course provides a deep dive into the design, execution, and analysis of controlled experiments in the context of performance engineering.

## Requirements

## Learning Outcomes

* Consider not only the speed of computational performance, but also the qualities of service, the total cost of ownership and facilities burden (space, power, and cooling).
* Differentiate types of benchmarks:
    * The Linpack benchmark is used, for example, to build the TOP 500 list of the currently fastest supercomputers, which is updated twice a year.
    * For HPC users, however, synthetic tests to benchmark HPC cluster hardware (like the Linpack benchmark) are of less importance, because the emphasis lies on the determination of speedups and efficiencies of the parallel program they want to use.
* Comprehend that benchmarking is very essential in the HPC environment and can be applied to a variety of issues:
    * What is the scalability of my program?
    * How many cluster nodes can be maximally used, before the efficiency drops to values which are unacceptable?
    * How does the same program perform in different cluster environments?
* Comprehend that benchmarking is also a basis for dealing with questions emerging from tuning, e.g.:
    * What is the appropriate task size (big vs. small) that may have a positive performance impact on my program?
    * Is the use of hyper-threading technology advantageous?
    * What is the best mapping of processes to nodes, pinning of processes/threads to CPUs or cores, and setting memory affinities to NUMA nodes in order to speed up a parallel program?
    * What is the best compiler selection for my program (GCC, Intel, PGI, ...), in combination with the most suitable MPI environment (Open MPI, Intel MPI, ...)?
    * What is the best compiler generation/version for my program?
    * What are the best compiler options regarding, for example, the optimization level -O2, -O3, . . . , for building the executable program?
    * Is the use of PGO (Profile Guided Optimization) or other high-level optimization, e.g. using IPA/IPO (Inter-Procedural Analyzer/Inter-Procedural Optimizer), helpful?
    * What is the performance behavior after a (parallel) algorithm has been improved, i.e. to what extent are speedup, efficiency, and scalability improved?
* Assess speedups and efficiencies as the key measures for benchmarks of a parallel program.
* Benchmark the runtime behaviour of parallel programs, performing controlled experiments by providing varying HPC resources (e.g. 1, 2, 4, 8, ... cores on shared memory systems or 1, 2, 4, 8, ... nodes on distributed systems for the benchmarks).
* Measure runtimes by the help of tools like:
    * Built-in time command, e.g. for MPI programs ('time mpirun ... my-mpi-app').
    * Stand-alone 'time' program, e.g. for sequential or OpenMP programs ('/usr/bin/time my-openmp-app').
